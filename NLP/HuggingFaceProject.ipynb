{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a328af",
   "metadata": {},
   "source": [
    "### 라이브러리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53429649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_metric, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d6526",
   "metadata": {},
   "source": [
    "# STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150dbd10",
   "metadata": {},
   "source": [
    "### NSMC(Naver sentiment movie corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba18c4",
   "metadata": {},
   "source": [
    "Each instance is a movie review written by Korean internet users on Naver, the most commonly used search engine in Korea. Each row can be broken down into the following fields:\n",
    "\n",
    "- id: A unique review ID, provided by Naver\n",
    "- document: The actual movie review\n",
    "- label: Binary labels for sentiment analysis, where 0 denotes negative, and 1, positive\n",
    "- train: 150000 rows / test: 50000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2222a20",
   "metadata": {},
   "source": [
    "### Huggingface dataset 구성\n",
    "train - 4000\n",
    "validation - 800\n",
    "test - 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2447d751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e37493701741f68c3aae406822cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22d62e42c724967a0f8ff1fca4a0644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default (download: 18.62 MiB, generated: 20.90 MiB, post-processed: Unknown size, total: 39.52 MiB) to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5644061735fd40368237684aa737f8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2477362c654488496dd31d8ef3ff96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05cb0cc9db94416ba066c0fe145b23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0711a6f5491d4ae9b13e3b5992bdcbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n",
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset('nsmc', split='train[:4000]')\n",
    "validation = load_dataset('nsmc', split='train[-800:]')\n",
    "test = load_dataset('nsmc', split='test[:1500]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76266545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a223792b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 개수: 4000, Validation Data 개수: 800, Test Data 개수: 1500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data 개수: {len(train)}, Validation Data 개수: {len(validation)}, Test Data 개수: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd3133",
   "metadata": {},
   "source": [
    "### 문장 길이가 maxlen 이하인 데이터 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15c8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_length(dataset, max_length=50):\n",
    "    filtered_dataset = dataset.filter(lambda example: len(example['document']) <= max_length)\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723e91aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475f076819c04fde8bebea85d395f486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0581783aca4d45b0a16fcc7ddfdb2170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac2a6faadfa448ab70967bc3a5d0bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = filter_dataset_by_length(train)\n",
    "validation = filter_dataset_by_length(validation)\n",
    "test = filter_dataset_by_length(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd741c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 최대 :  50\n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(train['document']) + list(validation['document']) + list(test['document'])\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print('문장길이 최대 : ', np.max(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a731b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 개수: 3255, Validation Data 개수: 671, Test Data 개수: 1242\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data 개수: {len(train)}, Validation Data 개수: {len(validation)}, Test Data 개수: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274f5d3",
   "metadata": {},
   "source": [
    "# STEP 2. klue/bert-base model 및 tokenizer 불러오기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d8b71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2308ae00a843d6b56326e4e944ace9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7454f7376a864209989fc44bad2e5f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5a1b3f964a44f89af3ecfce9f512eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271b0f6acd30480ab020d0d6c890c49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12ef85d850e4d6f918fcabc8c94fee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a92cc6a2afe4f54ab022779b5214e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30544037",
   "metadata": {},
   "source": [
    "# STEP 3. 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a941e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a9901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179a03cad89b439497204fd438397a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5191471beb4d809f5c86d4cfdb1d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d50c59e599f47d0b18bcf8ecb6c257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_train_dataset = train.map(transform, batched=True)\n",
    "hf_val_dataset = validation.map(transform, batched=True)\n",
    "hf_test_dataset = test.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e8ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/NLP/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                        #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514d67f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aee598722aa4dd9a2f1614e69cbe07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5435d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 3255\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1221\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1221' max='1221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1221/1221 17:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391954</td>\n",
       "      <td>0.845007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.654888</td>\n",
       "      <td>0.833085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>0.748196</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1221, training_loss=0.2811355153426687, metrics={'train_runtime': 1063.1561, 'train_samples_per_second': 9.185, 'train_steps_per_second': 1.148, 'total_flos': 2569279455590400.0, 'train_loss': 0.2811355153426687, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62142df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1242\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inital_result = trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc347202",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "results_dict['Initial_model'] = inital_result['eval_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d236aab",
   "metadata": {},
   "source": [
    "# STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c0e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 전 할당된 메모리: 1833351168\n",
      "실행 후 할당된 메모리: 1833351168\n",
      "최대 할당된 메모리 (실행 전): 8338522624\n",
      "최대 할당된 메모리 (실행 후): 8338522624\n"
     ]
    }
   ],
   "source": [
    "# 실행 전 메모리 사용량 측정\n",
    "initial_memory_allocated = torch.cuda.memory_allocated()\n",
    "initial_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# 가비지 컬렉션 수행\n",
    "gc.collect()\n",
    "\n",
    "# CUDA 캐시된 메모리 비우기\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 모델과 토크나이저 버퍼 비우기\n",
    "del huggingface_model, huggingface_tokenizer\n",
    "\n",
    "# 실행 후 메모리 사용량 측정\n",
    "final_memory_allocated = torch.cuda.memory_allocated()\n",
    "final_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# 메모리 사용량 출력\n",
    "print(\"실행 전 할당된 메모리:\", initial_memory_allocated)\n",
    "print(\"실행 후 할당된 메모리:\", final_memory_allocated)\n",
    "print(\"최대 할당된 메모리 (실행 전):\", initial_max_memory_allocated)\n",
    "print(\"최대 할당된 메모리 (실행 후):\", final_max_memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddfc1603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1908408320"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80ce3183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84337a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(data):\n",
    "#     return huggingface_tokenizer(\n",
    "#         data['document'],\n",
    "#         truncation = True,\n",
    "#         padding = 'max_length',\n",
    "#         return_token_type_ids = False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6398cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_train_dataset = train.map(transform, batched=True)\n",
    "# hf_val_dataset = validation.map(transform, batched=True)\n",
    "# hf_test_dataset = test.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efebef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/NLP/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                        #learning_rate\n",
    "    per_device_train_batch_size = 10,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 10,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 7,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30056086",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc1691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 3255\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2282' max='2282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2282/2282 39:32, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375724</td>\n",
       "      <td>0.834575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.550137</td>\n",
       "      <td>0.827124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.843699</td>\n",
       "      <td>0.833085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.792582</td>\n",
       "      <td>0.843517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.979478</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>1.023596</td>\n",
       "      <td>0.837556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>1.020963</td>\n",
       "      <td>0.845007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 10\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2282, training_loss=0.14009410994393484, metrics={'train_runtime': 2373.7788, 'train_samples_per_second': 9.599, 'train_steps_per_second': 0.961, 'total_flos': 5994985396377600.0, 'train_loss': 0.14009410994393484, 'epoch': 7.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a0a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1242\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_result = trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccc39192",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['FineTuned_model'] = finetuned_result['eval_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444e39f",
   "metadata": {},
   "source": [
    "# STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06937ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 전 할당된 메모리: 3572069376\n",
      "실행 후 할당된 메모리: 1788524544\n",
      "최대 할당된 메모리 (실행 전): 11740495360\n",
      "최대 할당된 메모리 (실행 후): 11740495360\n"
     ]
    }
   ],
   "source": [
    "# 실행 전 메모리 사용량 측정\n",
    "initial_memory_allocated = torch.cuda.memory_allocated()\n",
    "initial_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# 가비지 컬렉션 수행\n",
    "gc.collect()\n",
    "\n",
    "# CUDA 캐시된 메모리 비우기\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 모델과 토크나이저 버퍼 비우기\n",
    "del huggingface_model, huggingface_tokenizer\n",
    "\n",
    "# 실행 후 메모리 사용량 측정\n",
    "final_memory_allocated = torch.cuda.memory_allocated()\n",
    "final_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# 메모리 사용량 출력\n",
    "print(\"실행 전 할당된 메모리:\", initial_memory_allocated)\n",
    "print(\"실행 후 할당된 메모리:\", final_memory_allocated)\n",
    "print(\"최대 할당된 메모리 (실행 전):\", initial_max_memory_allocated)\n",
    "print(\"최대 할당된 메모리 (실행 후):\", final_max_memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a34f253c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4bfa53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(data):\n",
    "#     return huggingface_tokenizer(\n",
    "#         data['document'],\n",
    "#         truncation = True,\n",
    "#         padding = 'max_length',\n",
    "#         return_token_type_ids = False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "793dd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_train_dataset = train.map(transform, batched=True)\n",
    "# hf_val_dataset = validation.map(transform, batched=True)\n",
    "# hf_test_dataset = test.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d958f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/NLP/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 3e-5,                        #learning_rate\n",
    "    per_device_train_batch_size = 12,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 12,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 7,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.02,                        # weight decay\n",
    "    group_by_length = True                       # bucketing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "457d4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60dce4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 3255\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1904' max='1904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1904/1904 40:52, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.381005</td>\n",
       "      <td>0.842027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.656339</td>\n",
       "      <td>0.812221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.811882</td>\n",
       "      <td>0.846498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.907390</td>\n",
       "      <td>0.845007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>1.146797</td>\n",
       "      <td>0.839046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.126576</td>\n",
       "      <td>0.837556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>1.147162</td>\n",
       "      <td>0.842027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /aiffel/aiffel/NLP/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/NLP/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/NLP/transformers/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 671\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1904, training_loss=0.12568470209586521, metrics={'train_runtime': 2453.5085, 'train_samples_per_second': 9.287, 'train_steps_per_second': 0.776, 'total_flos': 5994985396377600.0, 'train_loss': 0.12568470209586521, 'epoch': 7.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "795fa121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1242\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/104 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucketing_result = trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c01a9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['Bucketing_model'] = bucketing_result['eval_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b653ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Initial_model': 0.8502415458937198,\n",
       " 'FineTuned_model': 0.8582930756843801,\n",
       " 'Bucketing_model': 0.8590982286634461}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d3f1cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdwElEQVR4nO3de5xWZd3v8c+XkyAIhKARiIyPoAihwmR4yDMkaLIzSilTyi25HxRFK33KLA+Vpm19eXh6MndKYirYVlFRNFAzBeUYpqjheYC9Qw4ajMIM/J4/1hq4GQa4hVn3MLO+79drXrMO11rrWvc193zX4b6vpYjAzMzyq1lDV8DMzBqWg8DMLOccBGZmOecgMDPLOQeBmVnOtWjoCnxanTt3jp49ezZ0NczMGpU5c+Z8EBFd6prX6IKgZ8+ezJ49u6GrYWbWqEh6d2vzfGnIzCznHARmZjnnIDADbrzxRvr27Uu/fv0YOXIkn3zyCaNGjaKsrIxDDjmEQw45hPnz59e57A9/+EP69u1Lnz59GDt2LDXf1j/ppJM4+OCD6du3L+eddx7r168H4NJLL6V///6cddZZG9cxYcIEbrrppqx3s0nZmTa79NJL6devH/369eP+++/fOP3WW29l//33RxIffPDBxul/+tOf6Nu3L1/60pdYvnw5AG+++Sann356pvtYMhHRqH4GDhwYZvWpoqIievbsGZWVlRER8fWvfz3uvPPOOPvss2PSpEnbXPb555+PI444Iqqrq6O6ujoGDRoUTz/9dEREfPjhhxERsWHDhjjttNPi3nvvjVWrVsWJJ54YERHnnHNOLFiwICorK+P444+PdevWZbeTTczOtNmjjz4aJ554YlRVVcXq1aujvLx8Y1vNnTs33n777dh3331j2bJlG5c55phjYs2aNXH33XfHzTffHBERZ5xxRrzxxhsZ7WH9A2bHVv6v+owgAztzpDJ+/Hh69epFr169GD9+PACVlZWcfPLJHHjggfTt25fLLrtsY/lbbrmFfv36MWzYMNatWwfAX//6V8aNG5f5fjYl1dXVfPzxx1RXV1NZWcnnPve5opaTxCeffMK6detYu3YtVVVV7L333gC0b99+47rXrVuHJJo1a0ZVVRURQWVlJS1btuSGG27gggsuoGXLlpntX1O0o2326quvcvTRR9OiRQvatm1L//79eeKJJwA49NBDqetTic2aNWPt2rUb2+y5557js5/9LL169arPXWo4W0uIXfVnVz8j2JkjleXLl0dZWVksX748VqxYEWVlZbFixYpYs2ZNTJ8+PSIi1q5dG0cddVRMmTIlIiK++MUvxvr16+Pqq6+OyZMnx4YNG2LIkCGxfPnybHe0ibnpppuibdu20blz5/jmN78ZERFnn3129O7dOz7/+c/HRRddFJ988kmdy15yySXRoUOHaN++ffzoRz/abN6QIUOiY8eOMXLkyKiuro6IiOuuuy4OPvjguPjii2PJkiVx8sknZ7tzTdSOttnUqVPjiCOOiDVr1sSyZcuirKwsbrjhhs3K1D4jePLJJ2PAgAFxyimnxKpVq2Lw4MGN7j2GzwhKa0ePVKZOncrgwYPp1KkTn/nMZxg8eDBPPPEEu+++O8cddxwArVq1YsCAAVRUVABJkFdVVW08UpkwYQJDhw6lU6dOme1fU7Ny5Uoefvhh3n77bZYsWcKaNWuYMGECv/zlL3nttdeYNWsWK1as4Lrrrtti2UWLFrFw4UIqKipYvHgx06dP57nnnts4f+rUqSxdupS1a9cyffp0ILmnMH/+fH7961/zk5/8hKuuuoo77riDb3zjG1xzzTUl2+/GbGfabMiQIQwbNowjjjiCkSNHcvjhh9O8efNtbm/w4MHMmTOHRx55hIcffphhw4bxxhtvMGLECM4991wqKyuz2tWScBDUs27duvH973+fHj160LVrVzp06MCQIUMA+PGPf0z//v0ZN24ca9eu3WLZxYsXs88++2wc7969O4sXL96szKpVq3jkkUc44YQTADj//PMZNGgQ7733HkceeSR33nknY8aMyXAPm54///nPlJWV0aVLF1q2bMlpp53GCy+8QNeuXZHEbrvtxne+8x1eeumlLZZ98MEHGTRoEO3ataNdu3YMHTqUGTNmbFamdevWDB8+nIcffniz6fPmzSMiOOCAA5g0aRITJ07kzTff5B//+Eem+9sU7EybQfJenD9/Pk899RQRQe/evYvabmVlJXfddRdjxozhpz/9KePHj+eoo47innvuqc/dKzkHQT3bmSOV7amurmbkyJGMHTuW/fbbD4Bvf/vbzJs3jwkTJnDjjTcyduxYHn/8cUaMGMG4cePYsGFDfe9ik9OjRw9mzpxJZWUlEcG0adPo06cPS5cuBZKzroceeoh+/frVueyzzz5LdXU1VVVVPPvss/Tp04fVq1dvXL66uprHHnuMAw88cLNlf/KTn3D11VdTVVW18RNFzZo1a/RHl6WwM222fv36jZ/8WbBgAQsWLNh4sLY9119/PWPHjqVly5Z8/PHHG+/7NPY2a3TfLN7VFR6pABuPVM4880yAjUcqN9xwwxbLduvWjWeeeWbjeEVFBccee+zG8dGjR9OrVy8uuuiiLZZdsmQJL730EldccQXHHHMM06dP55prrmHatGkMHjy4XvexlHpe9lhJtrOq48F02OcA1KwZrfb+N57p8lV+MOgkNlR+CASt9tqPTl8ew32XPcbapf9g9fzH2XPoWGJDG1Z80JI2e5chidZlA7jg+Wasf/Ih/vnAVcT6KogNtO7Rn5n79uCGdH8q35jBulXtOeLmeQCs/Kg9rbr0pOVePZnWpQLurch8n9+59uRM1rvLt1n1OpbedSEAarU7e355DPtfPhWAj2ZP5qMX/8T6NSv5bNkBtNmvnD2HjgWg+l/LWfHEo+z19XIumvkYazodSccefWjWui1dTrucG0uw31m1maKRPaGsvLw8duUuJl588UW++93vMmvWLNq0acOoUaMoLy9nxIgRdO3alYhg3LhxtG7dmmuvvXazZVesWMHAgQOZO3cuAAMGDGDOnDl06tSJyy+/nIULFzJp0iSaNdvyRO6cc85hzJgxDBgwgMMOO4yZM2fy85//nP79+zN8+PCS7HsWSvVPJY8aexDk0c60maQ5EVFe17xcnRHs6kcqAKv7nMpe+/UFoMPh32DAr2ZQ/dEHLP7Nz2nRqTutu+4PwB4DTmGPg78MwLr//yYfzXmfaROXwsTH+GiPQ2i9VxnN23dmr9M+z4UzGu+RipllL1dnBD5SyY6PLhsft1njk9UZgW8Wm5nlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzmQaBpJMkvS5pkaTL6pjfQ9LTkuZJWiBpWJb1MTOzLWUWBJKaA7cBQ4GDgJGSDqpV7HJgYkQcCpwB/GdW9TEzs7pleUZwGLAoIt6KiHXAfUDtTm8CaJ8OdwCWZFgfMzOrQ5ZB0A14v2C8Ip1W6GfAmZIqgCnABXWtSNJoSbMlzV62bFkWdTUzy62Gvlk8ErgrIroDw4C7JW1Rp4i4PSLKI6K8pntnMzOrH1kGwWJgn4Lx7um0QucAEwEiYgbQGuicYZ3MzKyWLINgFtBLUpmkViQ3gyfXKvMecAKApD4kQeBrP2ZmJZRZEERENXA+MBVYSPLpoFckXSXp1LTYJcC5kv4G3AuMisbWL7aZWSOX6YNpImIKyU3gwmlXFAy/ChyZZR3MzGzbGvpmsZmZNTAHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOcyDQJJJ0l6XdIiSZdtpcw3JL0q6RVJf8yyPmZmtqUWWa1YUnPgNmAwUAHMkjQ5Il4tKNML+A/gyIhYKWmvrOpjZmZ1y/KM4DBgUUS8FRHrgPuA4bXKnAvcFhErASLinxnWx8zM6pBlEHQD3i8Yr0inFeoN9Jb0vKSZkk6qa0WSRkuaLWn2smXLMqqumVk+NfTN4hZAL+BYYCTwO0kdaxeKiNsjojwiyrt06VLaGpqZNXFZBsFiYJ+C8e7ptEIVwOSIqIqIt4E3SILBzMxKJMsgmAX0klQmqRVwBjC5VpmHSM4GkNSZ5FLRWxnWyczMasksCCKiGjgfmAosBCZGxCuSrpJ0alpsKrBc0qvA08APImJ5VnUyM7MtZfbxUYCImAJMqTXtioLhAC5Of8zMrAE09M1iMzNrYA4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznNtuEEj6iiQHhplZE1XMP/jTgX9I+pWkA7OukJmZldZ2gyAizgQOBd4E7pI0I31i2B6Z187MzDJX1CWfiPgIeIDkucNdga8CcyVdkGHdzMysBIq5R3CqpAeBZ4CWwGERMRQ4GLgk2+qZmVnWinkewdeAGyPiL4UTI6JS0jnZVMvMzEqlmCD4GbC0ZkRSG2DviHgnIqZlVTEzMyuNYu4RTAI2FIyvT6eZmVkTUEwQtIiIdTUj6XCr7KpkZmalVEwQLCt42DyShgMfZFclMzMrpWLuEZwH3CPpVkDA+8BZmdbKzMxKZrtBEBFvAoMktUvHV2deKzMzK5lizgiQdDLQF2gtCYCIuCrDepmZWYkU84Wy/yLpb+gCkktDXwf2zbheZmZWIsXcLD4iIs4CVkbElcDhQO9sq2VmZqVSTBB8kv6ulPQ5oIqkvyEzM2sCirlH8IikjsD1wFwggN9lWSkzMyudbQZB+kCaaRGxCviTpEeB1hHxYSkqZ2Zm2dvmpaGI2ADcVjC+1iFgZta0FHOPYJqkr6nmc6NmZtakFBME3yPpZG6tpI8k/UvSRxnXy8zMSqSYbxb7kZRmZk3YdoNA0tF1Ta/9oBozM2ucivn46A8KhlsDhwFzgOMzqZGZmZVUMZeGvlI4Lmkf4KasKmRmZqVVzM3i2iqAPvVdETMzaxjF3CO4heTbxJAExyEk3zA2M7MmoJh7BLMLhquBeyPi+YzqY2ZmJVZMEDwAfBIR6wEkNZe0e0RUZls1MzMrhaK+WQy0KRhvA/y5mJVLOknS65IWSbpsG+W+JikklRezXjMzqz/FBEHrwsdTpsO7b28hSc1J+ikaChwEjJR0UB3l9gAuBF4sttJmZlZ/igmCNZIG1IxIGgh8XMRyhwGLIuKtiFgH3AcMr6Pc1cB1bHrugZmZlVAx9wguAiZJWkLyqMrPkjy6cnu6Ae8XjFcAXywskAbMPhHxmKTCL65Rq9xoYDRAjx49iti0mZkVq5gvlM2SdCBwQDrp9Yio2tkNp886+N/AqCLqcDtwO0B5eXlsp7iZmX0KxTy8fgzQNiL+HhF/B9pJ+vci1r0Y2KdgvHs6rcYeQD/gGUnvAIOAyb5hbGZWWsXcIzg3fUIZABGxEji3iOVmAb0klUlqBZwBTC5Yz4cR0TkiekZET2AmcGpEzK57dWZmloVigqB54UNp0k8DtdreQhFRDZwPTAUWAhMj4hVJV0k6dUcrbGZm9auYm8VPAPdL+m06/j3g8WJWHhFTgCm1pl2xlbLHFrNOMzOrX8UEwaUkn9g5Lx1fQPLJITMzawK2e2kofYD9i8A7JN8NOJ7kUo+ZmTUBWz0jkNQbGJn+fADcDxARx5WmamZmVgrbujT0GvAccEpELAKQNK4ktTIzs5LZ1qWh04ClwNOSfifpBJJvFpuZWROy1SCIiIci4gzgQOBpkq4m9pL0G0lDSlQ/MzPLWDE3i9dExB/TZxd3B+aRfJLIzMyagE/1zOKIWBkRt0fECVlVyMzMSmtHHl5vZmZNiIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzmQaBpJMkvS5pkaTL6ph/saRXJS2QNE3SvlnWx8zMtpRZEEhqDtwGDAUOAkZKOqhWsXlAeUT0Bx4AfpVVfczMrG5ZnhEcBiyKiLciYh1wHzC8sEBEPB0RlenoTKB7hvUxM7M6ZBkE3YD3C8Yr0mlbcw7weIb1MTOzOrRo6AoASDoTKAeO2cr80cBogB49epSwZmZmTV+WZwSLgX0Kxrun0zYj6UTgx8CpEbG2rhVFxO0RUR4R5V26dMmksmZmeZVlEMwCekkqk9QKOAOYXFhA0qHAb0lC4J8Z1sXMzLYisyCIiGrgfGAqsBCYGBGvSLpK0qlpseuBdsAkSfMlTd7K6szMLCOZ3iOIiCnAlFrTrigYPjHL7ZuZ2fb5m8VmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOcyDQJJJ0l6XdIiSZfVMX83Sfen81+U1DPL+piZ2ZYyCwJJzYHbgKHAQcBISQfVKnYOsDIi9gduBK7Lqj5mZla3LM8IDgMWRcRbEbEOuA8YXqvMcGB8OvwAcIIkZVgnMzOrpUWG6+4GvF8wXgF8cWtlIqJa0ofAnsAHhYUkjQZGp6OrJb2eSY13PZ2p9VrsquRzOWhE7QVus1Se2mzfrc3IMgjqTUTcDtze0PUoNUmzI6K8oethxXF7NT5us0SWl4YWA/sUjHdPp9VZRlILoAOwPMM6mZlZLVkGwSygl6QySa2AM4DJtcpMBs5Oh0cA0yMiMqyTmZnVktmlofSa//nAVKA58PuIeEXSVcDsiJgM/B/gbkmLgBUkYWGb5O5yWCPn9mp83GaAfABuZpZv/maxmVnOOQjMzHLOQWBmlnMOgu2QtLqIMnfUdJ8h6Ue15r1QH9uoL0XuT8nqUwxJ6yXNL/jpWczrupV13Zau41VJHxesc0R91zvd3jOSSvI59WK2Var6FLTZ3yTNlXTEDq5nlKRbP0X5iyTtXjA+RVLHHdl21tK/47/vbJn60Ci+ULari4j/WTD6I+AXBfN26A1gm/k4Ig6pNW2HXteIGAPJGwx4tI71Wv3Y2GaSvgz8EjimBNu9CJgAVAJExLASbLPR8xlBkSQdmx5NPSDpNUn31PSLVHOUJelaoE16JHRPOm91+rudpGnp0dHLkmr3u7St7T4r6WFJb0m6VtK3JL2Uruff0nI9JU2XtCDdTo90epmkGWnZa2qt+weSZqXLXFmPL1fmCl7XbbXLwPS1myNpqqSuW1nXsZIeLRi/VdKodPgdSVcWtNuB6fS2kn6ftsO8mvaU1EbSfZIWSnoQaLO9/ZB0vaRXJP1Z0mHp/rwl6dS0TGtJd6bbnyfpuO1tS9KQtN3nSpokqd0Ov9g7rz2wMq3Xtl7rL0h6IT2LeEnSHoUrkXRyuk+d69o/SWOBzwFPS3o6XeadtHzP9HX6XfpaPympTcF2F6Tv2+u1jSNwJWcoD0l6Kl33+ZIuTttlpqROablD0vEFkh6U9Jl0+sB0//4GjClYb/N02zXvx+/VyytfrIjwzzZ+gNXp72OBD0m+Id0MmAEclc57BigvLF/H8i2A9ulwZ2ARmz6+u3ob2z8WWAV0BXYj+Tb2lem8C4Gb0uFHgLPT4e8CD6XDk4Gz0uExBfUZQvIZaqX78yhw9Pbq00BtsB6Yn/48WEy7AC2BF4AuabnTSb7LUrPOnsDfC9bxaMG8W4FR6fA7wAXp8L8Dd6TDvwDOTIc7Am8AbYGLa7YD9Aeqa/42trJvAQxNhx8EnkzrfjAwP51+ScE6DwTeA1pvbVvp39dfgLbpvEuBK2r/rZaozV5L22fgtl5roBXwFvCFdHp7kvfMqLTMV4HngM9sZ//eAToXrP+dtHzP9PU5JJ0+saD9/g4cng5fW/N3sZX9GkXy3t0D6JLu23npvBuBi9LhBcAx6fBVbHqfLmDT++x6Nv0NjgYuT4d3A2YDZRT8nWb540tDn85LEVEBIGk+SSP9tchlBfxC0tHABpIO9/YG/l8Ry86KiKXpdt8k+WcB8DJwXDp8OHBaOnw38Kt0+EjgawXTa7qtGpL+zEvH2wG9SN5gu5q6Lg0VqqtdVgH9gKfSE4TmwNId3P7/TX/PYdNrPAQ4VdL30/HWQA/gaOBmgIhYIGnBdta9DngiHX4ZWBsRVZJeTvcDkmC7JV3na5LeBXpvY1uDSLp+fz7d91YkAVlKhZeGDgf+IKnfNsofACyNiFkAEfFRuizA8SQBNyQiPpJ0Cju2f29HxPx0eA7QU8n9gz0iomb5PwKnbGc9T0fEv4B/Keko85F0+stAf0kdgI4R8Ww6fTwwKd1Wx4ioeY/dTdJNPyR/T/216V5VB5L34xtF7NdOcxB8OmsLhtfz6V6/b5EcQQxM3+jvkPzz+LTb3VAwvqHIOtT1rUEBv4yI3xZZh11ZXe0i4JWIOLyI5avZ/DJp7XapWX9hmwv4WkRs1hOuPn0v6lWRHgZS0LYRsUFJ/1s7QsBTETFyB5evVxExQ1Jnkr//7b3WdXkT2I8k/Gaz4/tX++9km5ftilzPjrwf6yKSM8+pm00s0cO6fI+g/lVJalnH9A7AP9MQOI5tdAm7g15gUxcd3yI5jQZ4vtb0GlOB79ZcO5bUTdJe9VynhvQ60CU9GkVSS0l9t1L2XeAgJU/M6wicUMT6pwIXSBvvRxyaTv8L8M10Wj+SSzY76znStpPUm+TM4/VtbGsmcKSk/dN5bdPlGoSS+yrNSTqU3Npr/TrQVdIX0mX2KAjCd0nOav+QtuG29u9fJJdtihIRq0iO7Gu6yN/pbm4i4kNgpaQvpZO+DTybbmuVpKPS6bXfj/+r5n+HpN6S2u5sXYrlM4L6dzuwQNLciChs6HuAR9JT/tkk107r0wXAnZJ+ACwDvpNOvxD4o6RLgYdrCkfEk5L6ADPS/2WrgTOBf9ZzvRpERKxLT7NvTk/VWwA3Aa/UUfZ9SRNJrhW/zabLZdtydbq+BZKapcudAvyGpB0WAgtJLkHsrP8EfpP+7VST3L9YK6nObUXEsvQG7L2SdkvXcTklusyQapNepoPkaPfsiFgP1Plap+11OnBLehP3Y+DEmpWll8S+BUwCvkJyrb6u/bsdeELSkoiouWy6PecAv5O0AXiW5Lr/zjob+C8lH2V9i03vx+8Av5cUbLrEC3AHyaXAuenBxTLgf9RDPYrivobMLNcktYuImk+hXQZ0jYgLG7haJeUzAjPLu5Ml/QfJ/8N3Sc42csVnBLsISZ8n+RRBobURUfvxntYISXqR5GOBhb4dES83RH1s25R8Ca72gyHfjoivNkR9suYgMDPLOX9qyMws5xwEZmY55yAwq0VSSJpQMN5C0jIV9JFT5HreSb9ItVNlzLLmIDDb0hqgX/p5doDBJH08mTVJDgKzuk0BTk6HRwL31syQ1CntgXJB2sNk/3T6nkp6tXxF0h0kX6SqWeZMJT1qzpf0W0nNS7kzZtviIDCr233AGZJak3Td8GLBvCuBeRHRn+T5E39Ip/8U+GtE9CXpSbSmK/A+JL2fHpl2xLaezbsXMGtQ/kKZWR3S3jx7kpwNTKk1+yjSHl0jYnp6JtCepDfQ09Lpj0lamZY/ARgIzEq782hDE+nKw5oGB4HZ1k0GbiDpQ3/PnViPgPER8R/1USmz+uZLQ2Zb93uShwDV/vZvYW+gxwIfpP3nF/YGOpTkISoA04ARNb27pvcY6rv3WbMd5jMCs61IH3Zzcx2zfkbSg+QCkmfjnp1Ov5KkR8xXSLoFfy9dz6uSLgeeTHsqrSJ5Wty72e6BWXHcxYSZWc750pCZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOfff32En1GBDwj8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting keys and values from the dictionary\n",
    "keys = list(results_dict.keys())\n",
    "values = list(results_dict.values())\n",
    "\n",
    "# Creating the bar graph\n",
    "plt.bar(keys, values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v, str(round(v*100,2))+'%', ha='center')\n",
    "\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91619623",
   "metadata": {},
   "source": [
    "## 결과 및 회고\n",
    "1. 데이터 불러오기 - 총 15만 개의 train 데이터 중 4천 개를 train 데이터로, 8백 개를 validation 데이터로 설정했다.     \n",
    "    총 5만 개의 test 데이터 중 1천5백 개를 test 데이터로 설정해서 불러왔따.\n",
    "2. 데이터 필터링 - 최대 문장 길이가 50인 문장만 필터링해서 Train Data 개수: 3255, Validation Data 개수: 671, Test Data 개수: 1242로 줄였다.\n",
    "3. Finetuning - batch size를 10으로 늘리고 epoch도 7로 늘렸다\n",
    "4. Bucketing - Finetuning과 같은 조건에서 group_by_length를 True로 설정했다.\n",
    "\n",
    "설정을 변경하니 Test Accuracy가 증가한 걸 보니 설정을 잘한 거 같다.   \n",
    "이번 프로젝트에서 가장 어려웠던 점은 첫번째가 메모리 할당 문제로 학습이 안 되는 것이었다.   \n",
    "이 문제는 한 모델의 학습과 테스트가 끝난 후 메모리 정리를 한 다음에 했더니 해결이 되었다.   \n",
    "모델을 사용할 때 batch size나 메모리를 잘 확인하면서 해야된다는 것을 배웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51a883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
